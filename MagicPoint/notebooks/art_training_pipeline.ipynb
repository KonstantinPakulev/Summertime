{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from MagicPoint.dataset.art_dataset import *\n",
    "from MagicPoint.model.magic_point import MagicPoint\n",
    "from common.model_utils import detector_loss, detector_metrics, filter_probabilities\n",
    "from common.utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "config = load_config('../configs/art_config_notebooks.yaml')\n",
    "data_config = config['data']\n",
    "model_config = config['model']\n",
    "experiment_config = config['experiment']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "set_seed(experiment_config['seed'])\n",
    "\n",
    "train_dataset = ArtificialDataset(available_modes[0], data_config)\n",
    "val_dataset = ArtificialDataset(available_modes[1], data_config)\n",
    "test_dataset = ArtificialDataset(available_modes[2], data_config)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, model_config['batch_size'], collate_fn=collate,\n",
    "                               shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, model_config['val_batch_size'], collate_fn=collate, \n",
    "                             shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, 1, collate_fn=collate, \n",
    "                             shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "epoch = 0\n",
    "model = MagicPoint(model_config).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=model_config['learning_rate'])\n",
    "\n",
    "if experiment_config['load_checkpoints']:\n",
    "    checkpoint_path = get_checkpoint_path(experiment_config, model_config, \n",
    "                                          experiment_config['load_checkpoint_iter'])\n",
    "    if checkpoint_path.exists():\n",
    "        epoch, model_sd, optimizer_sd = load_checkpoint(checkpoint_path)\n",
    "        model.load_state_dict(model_sd)\n",
    "        optimizer.load_state_dict(optimizer_sd)\n",
    "\n",
    "writer = SummaryWriter(log_dir=get_logs_path(experiment_config))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "for epoch in range(epoch, experiment_config['num_epochs']):\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_precision = 0\n",
    "    train_recall = 0\n",
    "    \n",
    "    for item in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(item[IMAGE].to(device))\n",
    "        \n",
    "        loss = detector_loss(y_pred['logits'].to(device), item[KEYPOINT_MAP].to(device), item[MASK].to(device), device, model_config)\n",
    "        probs = filter_probabilities(y_pred['probs'], model_config)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        metrics = detector_metrics(probs, item[KEYPOINT_MAP].to(device))\n",
    "        \n",
    "        train_loss += loss.cpu().item()\n",
    "        train_precision += metrics['precision'].cpu().item()\n",
    "        train_recall += metrics['recall'].cpu().item()\n",
    "        \n",
    "    train_loss /= train_data_loader.__len__()\n",
    "    train_precision /= train_data_loader.__len__()\n",
    "    train_recall /= train_data_loader.__len__()\n",
    "    \n",
    "    writer.add_scalar('training/loss', train_loss, epoch)\n",
    "    writer.add_scalar('training/precision', train_precision, epoch)\n",
    "    writer.add_scalar('training/recall', train_recall, epoch)\n",
    "        \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_precision = 0\n",
    "        val_recall = 0\n",
    "        \n",
    "        for item in val_data_loader:\n",
    "            y_pred = model(item[IMAGE].to(device))\n",
    "            \n",
    "            loss = detector_loss(y_pred['logits'].to(device), item[KEYPOINT_MAP].to(device), item[MASK].to(device), device, model_config)\n",
    "            probs = filter_probabilities(y_pred['probs'], model_config)\n",
    "            \n",
    "            metrics = detector_metrics(probs, item[KEYPOINT_MAP].to(device))\n",
    "            \n",
    "            val_loss += loss.cpu().item()\n",
    "            val_precision += metrics['precision'].cpu().item()\n",
    "            val_recall += metrics['recall'].cpu().item()\n",
    "            \n",
    "        val_loss /= val_data_loader.__len__()\n",
    "        val_precision /= val_data_loader.__len__()\n",
    "        val_recall /= val_data_loader.__len__()\n",
    "            \n",
    "        writer.add_scalar('validation/loss', val_loss, epoch)\n",
    "        writer.add_scalar('validation/precision', val_precision, epoch)\n",
    "        writer.add_scalar('validation/recall', val_recall, epoch)\n",
    "    \n",
    "    if experiment_config['keep_checkpoints'] != 0 and epoch != 0 and epoch % experiment_config['save_interval'] == 0:\n",
    "        checkpoint_path = get_checkpoint_path(experiment_config, model_config, epoch)\n",
    "        save_checkpoint(epoch, model, optimizer, checkpoint_path)\n",
    "        clear_old_checkpoints(experiment_config)\n",
    "    \n",
    "writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-95606ff0",
   "language": "python",
   "display_name": "PyCharm (Summertime)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}