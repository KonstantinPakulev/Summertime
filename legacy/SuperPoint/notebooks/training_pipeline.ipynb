{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from SuperPoint.dataset.tum_dataset import TUMDataset, collate as tum_collate\n",
    "from SuperPoint.model.super_point import SuperPoint\n",
    "\n",
    "from common.model_utils import detector_loss, descriptor_loss, detector_metrics, filter_probabilities\n",
    "from common.utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "config = load_config('../configs/config_notebooks.yaml')\n",
    "data_config = config['data']\n",
    "model_config = config['model']\n",
    "experiment_config = config['experiment']\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "set_seed(experiment_config['seed'])\n",
    "\n",
    "train_dataset = TUMDataset(TRAINING, data_config)\n",
    "val_dataset = TUMDataset(VALIDATION, data_config, train_ratio=0.9)\n",
    "\n",
    "train_data_loader = DataLoader(train_dataset, model_config['batch_size'], collate_fn=tum_collate, shuffle=True)\n",
    "val_data_loader = DataLoader(val_dataset, model_config['eval_batch_size'], collate_fn=tum_collate, shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "epoch = 0\n",
    "model = SuperPoint(model_config).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=model_config['learning_rate'])\n",
    "\n",
    "if experiment_config['load_checkpoints']:\n",
    "    checkpoint_path = get_checkpoint_path(experiment_config, model_config, \n",
    "                                          experiment_config['load_checkpoint_iter'])\n",
    "    if checkpoint_path.exists():\n",
    "        epoch, model_sd, optimizer_sd = load_checkpoint(checkpoint_path)\n",
    "        model.load_state_dict(model_sd)\n",
    "        optimizer.load_state_dict(optimizer_sd)\n",
    "\n",
    "writer = SummaryWriter(log_dir=get_logs_path(experiment_config))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "for epoch in range(epoch, experiment_config['num_epochs']):\n",
    "    model.train()\n",
    "    \n",
    "    train_total_det_loss = 0\n",
    "    train_det_loss = 0\n",
    "    train_warped_det_loss = 0\n",
    "    \n",
    "    train_desc_loss = 0\n",
    "    \n",
    "    train_det_precision = 0\n",
    "    train_det_recall = 0\n",
    "    \n",
    "    train_warped_det_precision = 0\n",
    "    train_warped_det_recall = 0\n",
    "    \n",
    "    for item in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(item[IMAGE].to(device))\n",
    "        y_warped_pred = model(item[WARPED_IMAGE].to(device))\n",
    "        \n",
    "        det_loss = detector_loss(y_pred['logits'].to(device), item[KEYPOINT_MAP].to(device), \n",
    "                             item[MASK].to(device), device, model_config)\n",
    "        warped_det_loss = detector_loss(y_warped_pred['logits'].to(device),\n",
    "                                        item[WARPED_KEYPOINT_MAP].to(device),\n",
    "                                        item[WARPED_MASK].to(device), device, model_config)\n",
    "        \n",
    "        total_det_loss = det_loss + warped_det_loss\n",
    "        \n",
    "        desc_loss = model_config['lambda_loss'] * descriptor_loss(y_pred['raw_desc'], y_warped_pred['raw_desc'], \n",
    "                                                            item[HOMOGRAPHY], item[WARPED_MASK].to(device),\n",
    "                                                            device,\n",
    "                                                            model_config)\n",
    "        \n",
    "        loss = total_det_loss + desc_loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        probs = filter_probabilities(y_pred['probs'], model_config)\n",
    "        warped_probs = filter_probabilities(y_warped_pred['probs'], model_config)\n",
    "        \n",
    "        metrics = detector_metrics(probs, item[KEYPOINT_MAP].to(device))\n",
    "        warped_metrics = detector_metrics(warped_probs, item[WARPED_KEYPOINT_MAP].to(device))\n",
    "        \n",
    "        train_total_det_loss += total_det_loss.cpu().item()\n",
    "        train_det_loss += det_loss.cpu().item()\n",
    "        train_warped_det_loss += warped_det_loss.cpu().item()\n",
    "        \n",
    "        train_desc_loss += desc_loss.cpu().item()\n",
    "        \n",
    "        train_det_precision += metrics['precision'].cpu().item()\n",
    "        train_det_recall += metrics['recall'].cpu().item()\n",
    "        \n",
    "        train_warped_det_precision += warped_metrics['precision'].cpu().item()\n",
    "        train_warped_det_recall += warped_metrics['recall'].cpu().item()\n",
    "        \n",
    "        break\n",
    "        \n",
    "    writer.add_scalar('training/total_det_loss', train_total_det_loss, epoch)\n",
    "    writer.add_scalar('training/det_loss', train_det_loss, epoch)\n",
    "    writer.add_scalar('training/warped_det_loss', train_warped_det_loss, epoch)\n",
    "    \n",
    "    writer.add_scalar('training/desc_loss', train_desc_loss, epoch)\n",
    "    \n",
    "    writer.add_scalar('training/det_precision', train_det_precision, epoch)\n",
    "    writer.add_scalar('training/det_recall', train_det_recall, epoch)\n",
    "    \n",
    "    writer.add_scalar('training/warped_det_precision', train_warped_det_precision, epoch)\n",
    "    writer.add_scalar('training/warped_det_recall', train_warped_det_recall, epoch)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        val_total_det_loss = 0\n",
    "        val_det_loss = 0\n",
    "        val_warped_det_loss = 0\n",
    "        \n",
    "        val_desc_loss = 0\n",
    "        \n",
    "        val_det_precision = 0\n",
    "        val_det_recall = 0\n",
    "        \n",
    "        val_warped_det_precision = 0\n",
    "        val_warped_det_recall = 0\n",
    "        \n",
    "        for item in val_data_loader:\n",
    "            y_pred = model(item[IMAGE].to(device))\n",
    "            y_warped_pred = model(item[WARPED_IMAGE].to(device))\n",
    "            \n",
    "            det_loss = detector_loss(y_pred['logits'].to(device), item[KEYPOINT_MAP].to(device), \n",
    "                                     item[MASK].to(device), device, model_config)\n",
    "            warped_det_loss = detector_loss(y_warped_pred['logits'].to(device),\n",
    "                                            item[WARPED_KEYPOINT_MAP].to(device),\n",
    "                                            item[WARPED_MASK].to(device), device, model_config)\n",
    "            \n",
    "            total_det_loss = det_loss + warped_det_loss\n",
    "            \n",
    "            desc_loss = model_config['lambda_loss'] * descriptor_loss(y_pred['raw_desc'], y_warped_pred['raw_desc'], \n",
    "                                                                      item[HOMOGRAPHY], item[WARPED_MASK].to(device),\n",
    "                                                                      device,\n",
    "                                                                      model_config)\n",
    "            \n",
    "            probs = filter_probabilities(y_pred['probs'], model_config)\n",
    "            warped_probs = filter_probabilities(y_warped_pred['probs'], model_config)\n",
    "            \n",
    "            metrics = detector_metrics(probs, item[KEYPOINT_MAP].to(device))\n",
    "            warped_metrics = detector_metrics(warped_probs, item[WARPED_KEYPOINT_MAP].to(device))\n",
    "            \n",
    "            val_total_det_loss += total_det_loss.cpu().item()\n",
    "            val_det_loss += det_loss.cpu().item()\n",
    "            val_warped_det_loss += warped_det_loss.cpu().item()\n",
    "        \n",
    "            val_desc_loss += desc_loss.cpu().item()\n",
    "            \n",
    "            val_det_precision += metrics['precision'].cpu().item()\n",
    "            val_det_recall += metrics['recall'].cpu().item()\n",
    "            \n",
    "            val_warped_det_precision += warped_metrics['precision'].cpu().item()\n",
    "            val_warped_det_recall += warped_metrics['recall'].cpu().item()\n",
    "            break\n",
    "\n",
    "        \n",
    "        writer.add_scalar('validation/total_det_loss', val_total_det_loss, epoch)\n",
    "        writer.add_scalar('validation/det_loss', val_det_loss, epoch)\n",
    "        writer.add_scalar('validation/warped_det_loss', val_warped_det_loss, epoch)\n",
    "\n",
    "        writer.add_scalar('validation/desc_loss', val_desc_loss, epoch)\n",
    "        writer.add_scalar('validation/det_precision', val_det_precision, epoch)\n",
    "        writer.add_scalar('validation/det_recall', val_det_recall, epoch)\n",
    "\n",
    "        writer.add_scalar('validation/warped_det_precision', val_warped_det_precision, epoch)\n",
    "        writer.add_scalar('validation/warped_det_recall', val_warped_det_recall, epoch)\n",
    "    \n",
    "    if experiment_config['keep_checkpoints'] != 0 and epoch != 0 and epoch % experiment_config['save_interval'] == 0:\n",
    "        checkpoint_path = get_checkpoint_path(experiment_config, model_config, epoch)\n",
    "        save_checkpoint(epoch, model, optimizer, checkpoint_path)\n",
    "        clear_old_checkpoints(experiment_config)\n",
    "    \n",
    "    \n",
    "writer.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "pycharm-95606ff0",
   "language": "python",
   "display_name": "PyCharm (Summertime)"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}